{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e27072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"/Users/aia04/Desktop/HG/test_llama/gpt-oss-20b-F16.gguf\" \n",
    "\n",
    "llm = Llama(\n",
    "  model_path=model_path,\n",
    "  n_gpu_layers=-1, \n",
    "  n_ctx=4096,     \n",
    "  verbose=False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d057f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O formato da saída que eu produzo pode variar dependendo do contexto e do que você pede. Aqui estão os principais formatos que eu uso com mais frequência:\n",
      "\n",
      "| Formato | Quando usar | Exemplo de saída |\n",
      "|---------|-------------|------------------|\n",
      "| **Texto puro** | Respostas curtas, explicações simples | `Claro, aqui está a resposta:` |\n",
      "| **Markdown** | Quando você quer que a resposta seja formatada (títulos, listas, código, etc.) | ```markdown<br># Título<br>• Item 1<br>• Item 2``` |\n",
      "| **Código** | Quando você pede um trecho de código em alguma linguagem | ```python<br>print(\"Hello, world!\")``` |\n",
      "| **JSON** | Quando você pede dados estruturados ou uma resposta em formato de objeto | ```json<br>{ \"nome\": \"João\", \"idade\": 30 }``` |\n",
      "| **HTML** | Quando você pede um trecho de página web ou algo que precise de tags | ```html<br><h1>Título</h1><p>Parágrafo</p>``` |\n",
      "| **LaTeX** | Quando você pede fórmulas matemáticas ou equações | ```latex<br>\\int_0^1 x^2 dx``` |\n",
      "\n",
      "### Como indicar o formato desejado\n",
      "\n",
      "- **Markdown**: Se você quiser que eu responda em Markdown, basta dizer algo como “responda em Markdown” ou “formate a resposta em Markdown”.\n",
      "- **Código**: Se precisar de um bloco de código, indique a linguagem, por exemplo: “mostre um exemplo em Python”.\n",
      "- **JSON**: Para dados estruturados, peça “retorne em JSON”.\n",
      "- **HTML**: Se precisar de tags HTML, peça “exemplo em HTML”.\n",
      "- **LaTeX**: Para fórmulas, peça “mostre a equação em LaTeX”.\n",
      "\n",
      "Se você não especificar, eu normalmente retorno em **texto puro** ou em **Markdown** (se houver formatação necessária). Se precisar de algo específico, é só me avisar!\n"
     ]
    }
   ],
   "source": [
    "prompt_do_usuario = \"\"\"\n",
    "qual o formato da saída dos seus prompts? markdown, html, outro?\n",
    "\"\"\"\n",
    "\n",
    "prompt_completo = f\"\"\"<|start|>user<|channel|>general<|message|>{prompt_do_usuario}<|end|><|start|>assistant<|channel|>final<|message|>\"\"\"\n",
    "\n",
    "output = llm(\n",
    "  prompt_completo,\n",
    "  max_tokens=2048,\n",
    "  #stop=[\"<|end|>\", \"<|user|>\"],\n",
    "  echo=False,\n",
    "  temperature=0.1 \n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
